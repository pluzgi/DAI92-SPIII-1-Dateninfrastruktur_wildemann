{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SP III-1 Prüfungsleistung - Einführung\n",
    "\n",
    "Es liegen Daten aus dem Zensus 2022 vor, die für Analysen vorbereitet werden sollen.\n",
    "Spezifisch soll die Auswertbarkeit nach Ländern und Regionalebene ermöglicht werden. Im Folgenden wird der ETL-Prozess dargestellt mit dem Zensusdaten aus einer Excel-Datei extrahiert, bereinigt und in ein Sternschema für eine PostgreSQL-Datenbank überführt werden. Der Fokus liegt darauf, eine Datenstruktur zu entwickeln, die schnelle und effiziente analytische Abfragen ermöglicht.\n",
    "Erste visuelle Analysen werden als Abschluss präsentiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dateninfastruktur aufsetzen\n",
    "\n",
    "1. PostgreSQL Server lokal aufsetzen\n",
    "- Zunächst erfolgt Authentifizierung und Herstellung einer Datenbanksitzung in PostgreSQL über die psql-Konsole.\n",
    "- PostgreSQL-Datenbank und Benutzer konfigurieren\n",
    "\n",
    "2. VS Code und Python\n",
    "3. pgAdmin für die Verwaltung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, Table, Column, Integer, String, Float, MetaData, ForeignKey, text\n",
    "import numpy as np\n",
    "from psycopg2 import sql\n",
    "import sweetviz as sv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import locale\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Überführung der Daten in eine relationale Datenbank "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Extract\n",
    "Extrahieren von 10 Tabellenblättern aus einer Excel-Datei, die in der Benennung mit \"CSV-\" beginnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basispfade und Dateinamen\n",
    "base_path = '/Users/sabinewildemann/Desktop/Studium/Pruefungsleistung/'\n",
    "input_path = os.path.join(base_path, 'input')\n",
    "output_path = os.path.join(base_path, 'output')\n",
    "file_name = \"Regionaltabelle_Bildung_Erwerbstaetigkeit 1.xlsx\"\n",
    "file_path = os.path.join(input_path, file_name)\n",
    "\n",
    "# Einlesen der Excel-Datei\n",
    "xls = pd.ExcelFile(file_path)\n",
    "\n",
    "# Einlesen und Laden der Attribute Details\n",
    "attribute_details_df = xls.parse('Attribute_Details')\n",
    "attribute_details_df['attribut_id'] = range(1, len(attribute_details_df) + 1)\n",
    "\n",
    "# Attribute Details in einem Dictionary speichern für schnellen Zugriff\n",
    "attribute_details_dict = dict(zip(attribute_details_df['Attribut'], attribute_details_df['attribut_id']))\n",
    "\n",
    "# Datenframes für die Daten und Attribute vorbereiten\n",
    "data_frames = {}\n",
    "\n",
    "# Festlegen, welche Sheets ausgelesen werden sollen\n",
    "sheets_to_read = [sheet for sheet in xls.sheet_names if sheet.startswith(\"CSV-\")]\n",
    "\n",
    "# Daten für jede Entität einlesen und verarbeiten\n",
    "for sheet_name in sheets_to_read:\n",
    "    df = xls.parse(sheet_name, dtype={'_RS': str})\n",
    "    clean_name = sheet_name.replace(\"CSV-\", \"\")\n",
    "    \n",
    "    # Definieren der Muster, die für Summenspalten stehen und daher ausgeschlossen werden sollen\n",
    "    summen_spalten_muster = ('_STP', '_STP__M', '_STP__W', '_ERW', '_ERW__M', '_ERW__W')\n",
    "\n",
    "    # Anpassen des DataFrame, indem alle Spalten, die auf die definierten Muster enden, ausgeschlossen werden\n",
    "    df = df[[col for col in df.columns if not col.endswith(summen_spalten_muster)]]\n",
    "    \n",
    "    # Hinzufügen von Attributdetails und Entität-Name\n",
    "    df['Entität'] = clean_name\n",
    "    for col in df.columns:\n",
    "        if col in attribute_details_dict:\n",
    "            df[col + '_detail'] = attribute_details_dict[col]\n",
    "\n",
    "    data_frames[clean_name] = df\n",
    "    \n",
    "# Anzahl der geladenen Tabellen und deren Namen ausgeben\n",
    "print(f\"Anzahl der geladenen Tabellen: {len(sheets_to_read)}\")\n",
    "print(f\"Geladene Tabellen und entsprechende DataFrame-Namen: {list(data_frames.keys())}\")\n",
    "\n",
    "# Überprüfung der Daten\n",
    "for sheet, df in data_frames.items():\n",
    "    print(f\"\\nDatenüberprüfung für {sheet}:\")\n",
    "    display(df.head(10))    # YY so kann man das df schöner darstellen, beachte dazu den import von display weiter oben\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing und Profiling report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lt. Erläuterungen zur Methodik in der Zensus 2022_Datei gibt es einige Sonderzeichen, die zu berücksichtigen sind:\n",
    "#–= Genau Null oder auf Null geändert\n",
    "#/= Keine Angabe, da Zahlenwert nicht sicher genug\n",
    "#.= Zahlenwert unbekannt oder geheim\n",
    "\n",
    "def replace_special_characters(val): \n",
    "    if isinstance(val, str):\n",
    "        # Entferne führende und nachfolgende Leerzeichen\n",
    "        val = val.strip()\n",
    "        \n",
    "        # Prüfe, ob der Wert ein einzelner langer Gedankenstrich ist\n",
    "        if val == '–':\n",
    "            return 0\n",
    "        elif val == '/' or val == '.':\n",
    "            return np.nan\n",
    "    \n",
    "    # Gibt den ursprünglichen Wert zurück, wenn keine Ersetzung erforderlich ist\n",
    "    return (val)\n",
    "\n",
    "def count_and_replace_special_characters(df):\n",
    "    special_chars = ['–', '.', '/']  # Verwende nur die relevanten Sonderzeichen\n",
    "    counts = {char: 0 for char in special_chars}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            # Zählen der Sonderzeichen\n",
    "            for char in special_chars:\n",
    "                counts[char] += df[col].apply(lambda x: str(x).strip() == char).sum()\n",
    "            \n",
    "            # Ersetzen der Sonderzeichen nur, wenn sie allein in einer Zelle stehen\n",
    "            # YY das aus dem Kommentar wird hier aber nicht gemacht\n",
    "            df[col] = df[col].apply(replace_special_characters)\n",
    "        else:\n",
    "            # Wenn die Spalte numerisch ist, gibt es keine Sonderzeichen\n",
    "            continue\n",
    "    \n",
    "    return counts\n",
    "\n",
    "# Überprüfe jedes Sheet, zähle die Sonderzeichen und ersetze sie\n",
    "for sheet, df in data_frames.items():\n",
    "    special_char_counts = count_and_replace_special_characters(df)\n",
    "    print(f\"Gefundene und ersetzte spezielle Zeichen im DataFrame {sheet}:\")\n",
    "    for char, count in special_char_counts.items():\n",
    "        print(f\"Anzahl von '{char}': {count}\")\n",
    "\n",
    "# Optional: Überprüfung der Ergebnisse\n",
    "for sheet_name, df in data_frames.items():\n",
    "    print(f\"\\nÜberprüfung des DataFrames '{sheet_name}':\")\n",
    "    display(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YY unnötig?\n",
    "\n",
    "def check_for_special_characters(df):\n",
    "    # Definiere die Sonderzeichen, die überprüft werden sollen\n",
    "    special_chars = ['–', '.', '/']\n",
    "    # Überprüfe nur die ersten fünf Spalten\n",
    "    for col in df.columns[:5]:\n",
    "        if df[col].dtype == object:\n",
    "            for char in special_chars:\n",
    "                if df[col].astype(str).apply(lambda x: x == char).any():\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "# Überprüfe alle DataFrames und gib aus, ob noch Sonderzeichen in den ersten fünf Spalten vorhanden sind\n",
    "for sheet_name, df in data_frames.items():\n",
    "    if check_for_special_characters(df):\n",
    "        print(f\"Sonderzeichen gefunden im DataFrame '{sheet_name}'!\")\n",
    "    else:\n",
    "        print(f\"Keine Sonderzeichen gefunden im DataFrame '{sheet_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Übersichtstabelle erstellen und Daten prüfen\n",
    "summary_data = []\n",
    "for sheet, df in data_frames.items():\n",
    "    # Reihenfolge der Spalten anpassen, um \"Entität\" als fünfte Spalte zu positionieren\n",
    "    # Die Spalte \"Entität\" sollte vor den numerischen Spalten stehen\n",
    "    column_order = ['_RS', 'Name', 'Regionalebene', 'Berichtszeitpunkt', 'Entität'] + [col for col in df.columns if col not in ['_RS', 'Name', 'Regionalebene', 'Berichtszeitpunkt', 'Entität']]\n",
    "    df = df[column_order]\n",
    "\n",
    "    # Spalten ab der sechsten Spalte in numerische Werte konvertieren, da \"Entität\" jetzt die fünfte Spalte ist\n",
    "    for col in df.columns[5:]:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')\n",
    "\n",
    "    # Nummer der Spalten und ihre Datentypen speichern\n",
    "    num_columns = df.shape[1]\n",
    "    dtypes = df.dtypes\n",
    "    dtypes_str = \", \".join([f\"{col}: {dtype}\" for col, dtype in dtypes.items()])\n",
    "\n",
    "    # Daten für die Zusammenfassungstabelle sammeln\n",
    "    summary_data.append([sheet, num_columns, dtypes_str])\n",
    "    print(f\"Sheet: {sheet}\")\n",
    "    display(df.head())\n",
    "\n",
    "# Zusammenfassungstabelle erstellen und ausgeben\n",
    "summary_df = pd.DataFrame(summary_data, columns=[\"Sheet Name\", \"Number of Columns\", \"Column Data Types\"])\n",
    "display(summary_df)\n",
    "\n",
    "# #Sweetviz-Berichte generieren\n",
    "# for sheet, df in data_frames.items():\n",
    "#     try:\n",
    "#         print(f\"Erzeuge Sweetviz-Bericht für das Tabellenblatt: {sheet}\")\n",
    "#         report = sv.analyze(df)\n",
    "#         report_file_path = os.path.join(output_path, f\"sweetviz_report_{sheet}.html\")\n",
    "#         report.show_html(report_file_path)\n",
    "#         print(f\"Sweetviz-Bericht gespeichert: {report_file_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Fehler bei der Erstellung des Sweetviz-Berichts für {sheet}: {e}\")\n",
    "\n",
    "# print(\"Sweetviz-Berichte wurden als HTML-Dateien gespeichert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transform\n",
    "Im Datenbereinigungs-Schritt werden folgende Anpassungen in den Spalten vorgenommen:\n",
    "- \"Name\" in Data type \"String\" konvertiert\n",
    "- \"Regionalebene\" in Data type \"String\" konvertiert und überschüssige Leerzeichen entfernt\n",
    "- \"Entität\" in Data type \"String\" konvertiert  \n",
    "- Alle Spaltenbezeichnungen in Kleinbuchstaben ändern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data_frames):\n",
    "    for sheet, df in data_frames.items():\n",
    "        print(f\"\\nVerarbeite Sheet: {sheet}\")\n",
    "        \n",
    "        # Konvertiere 'Name' und 'Regionalebene' zu Strings und normalisiere sie\n",
    "        if 'Name' in df.columns:\n",
    "            df['Name'] = df['Name'].astype(str)\n",
    "            print(\"Name konvertiert zu String.\")\n",
    "        \n",
    "        if 'Regionalebene' in df.columns:\n",
    "            df['Regionalebene'] = df['Regionalebene'].astype(str)\n",
    "            df['Regionalebene'] = df['Regionalebene'].str.strip()\n",
    "            print(\"Regionalebene konvertiert zu String und normalisiert.\")\n",
    "\n",
    "        if 'Entität' in df.columns:\n",
    "            df['Entität'] = df['Entität'].astype(str)\n",
    "            print(\"Entität konvertiert zu String.\")\n",
    "\n",
    "        # Ändere alle Spaltennamen in Kleinbuchstaben\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "        print(f\"Spaltennamen in '{sheet}' zu Kleinbuchstaben geändert.\")\n",
    "\n",
    "        # Speichere den veränderten DataFrame zurück in das Dictionary\n",
    "        data_frames[sheet] = df\n",
    "\n",
    "    print(\"\\nTransformation abgeschlossen.\")\n",
    "    \n",
    "    # Rückgabe des aktualisierten Dictionaries zur weiteren Verwendung oder Überprüfung\n",
    "    return data_frames\n",
    "\n",
    "# Anwendung der Transformation und Speichern des Ergebnisses\n",
    "transformed_data_frames = transform_data(data_frames)\n",
    "\n",
    "# Drucke die Spaltennamen für jede Tabelle zur Überprüfung\n",
    "for sheet, df in transformed_data_frames.items():\n",
    "    print(f\"Spaltennamen in '{sheet}': {df.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Überprüfung der Summendaten der Regionalebenen\n",
    "def verify_sum_data(data_frames):\n",
    "    # Liste der Regionalebenen\n",
    "    region_levels = ['Gemeinde', 'Gemeindeverband', 'Stadtkreis/kreisfreie Stadt/Landkreis', 'Regierungsbezirk', 'Land', 'Bund']\n",
    "\n",
    "    # Berechnung der Summen für jede Regionalebene\n",
    "    for sheet, df in data_frames.items():\n",
    "        print(f\"\\nBerechnungen für Sheet: {sheet}\")\n",
    "        for level in region_levels:\n",
    "            if 'regionalebene' in df.columns:\n",
    "                # Der Index für die Spalte direkt rechts von 'Regionalebene'\n",
    "                region_index = df.columns.get_loc('regionalebene')\n",
    "                sum_column_name = df.columns[region_index + 1]  # Nimmt die Spalte direkt nach 'Regionalebene'\n",
    "                \n",
    "                # Filtern der Daten nach der Regionalebene und Überprüfen der Summen\n",
    "                filtered_df = df[df['regionalebene'] == level]\n",
    "                if not filtered_df.empty:\n",
    "                    level_sum = filtered_df[sum_column_name].sum()\n",
    "                    print(f\"Gesamtsumme für {level} in {sheet}: {level_sum}\")\n",
    "                else:\n",
    "                    print(f\"Keine Daten gefunden für {level} in {sheet}\")\n",
    "\n",
    "verify_sum_data(data_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überprüfung der Verteilung der Regionalebenen in allen DataFrames\n",
    "region_level_distribution = {}\n",
    "for sheet, df in data_frames.items():\n",
    "    distribution = df['regionalebene'].value_counts(normalize=True, sort=False).sort_index()\n",
    "    region_level_distribution[sheet] = distribution\n",
    "\n",
    "# Vergleichen der Verteilungen\n",
    "base_distribution = None\n",
    "consistent = True\n",
    "for sheet, dist in region_level_distribution.items():\n",
    "    if base_distribution is None:\n",
    "        base_distribution = dist\n",
    "    elif not dist.equals(base_distribution):\n",
    "        consistent = False\n",
    "        break\n",
    "\n",
    "if consistent:\n",
    "    print(\"Die Verteilung der Regionalebenen ist über alle DataFrames hinweg konsistent.\")\n",
    "else:\n",
    "    print(\"Die Verteilung der Regionalebenen ist nicht konsistent.\")\n",
    "\n",
    "# Ausgabe der Verteilung für alle DataFrames\n",
    "for sheet, dist in region_level_distribution.items():\n",
    "    print(f\"Verteilung für {sheet}:\\n{dist}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse der Struktur des Regionalschlüssels '_RS' im Zusammenhang mit der 'Regionalebene'\n",
    "rs_structure = {}\n",
    "for sheet, df in data_frames.items():\n",
    "    rs_length = df.apply(lambda row: (row['regionalebene'], len(str(row['_rs']))), axis=1)\n",
    "    rs_structure[sheet] = rs_length.value_counts()\n",
    "\n",
    "# Überprüfung der Konsistenz der Struktur\n",
    "base_structure = None\n",
    "structure_consistent = True\n",
    "for sheet, structure in rs_structure.items():\n",
    "    if base_structure is None:\n",
    "        base_structure = structure\n",
    "    elif not structure.equals(base_structure):\n",
    "        structure_consistent = False\n",
    "        break\n",
    "\n",
    "if structure_consistent:\n",
    "    print(\"Die Struktur des Regionalschlüssels ist über alle DataFrames hinweg konsistent.\")\n",
    "else:\n",
    "    print(\"Die Struktur des Regionalschlüssels ist nicht konsistent.\")\n",
    "\n",
    "# Ausgabe der Strukturen für alle DataFrames\n",
    "for sheet, structure in rs_structure.items():\n",
    "    print(f\"Struktur für {sheet}:\\n{structure}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regionalebenen 'Gemeinde', 'Gemeindeverband', 'Regierungsbezirk', 'Bund' entfernen, um Redundanz und unvollständiefe Daten zu vermeiden\n",
    "def remove_specific_region_levels(data_frames):\n",
    "    # Liste der zu entfernenden Regionalebenen\n",
    "    levels_to_remove = ['Gemeinde', 'Gemeindeverband', 'Regierungsbezirk', 'Bund']\n",
    "\n",
    "    for sheet_name, df in data_frames.items():\n",
    "        if 'regionalebene' in df.columns:\n",
    "            # Entferne die Zeilen, die die spezifischen Regionalebenen enthalten\n",
    "            df = df[~df['regionalebene'].isin(levels_to_remove)]\n",
    "            data_frames[sheet_name] = df  # Aktualisiere den DataFrame im Dictionary\n",
    "            print(f\"Die Regionalebenen {levels_to_remove} wurden aus dem DataFrame '{sheet_name}' entfernt.\")\n",
    "\n",
    "# Wende die Funktion auf die DataFrames an\n",
    "remove_specific_region_levels(data_frames)\n",
    "\n",
    "# Optional: Überprüfung der Ergebnisse\n",
    "for sheet_name, df in data_frames.items():\n",
    "    print(f\"\\nÜberprüfung des DataFrames '{sheet_name}':\")\n",
    "    print(df['regionalebene'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Überführung in ein Sternschema\n",
    "### 3a: Design eines Sternschemas mit Dimensionstabellen und einer Faktentabelle\n",
    "### 3b: Überführen der Daten in das Sternschema - Entpivotieren\n",
    "\n",
    "Nach dem Entpivotieren werden die Dimensionen in separate Tabellen extrahiert und durch referenzielle IDs in der Faktentabelle ersetzt. Dies wird durch die Erstellung eines eindeutigen Identifikators für jede Dimension erreicht und dann die Zuordnung dieser IDs in der Faktentabelle.\n",
    "Die Daten werden von einem breiten zu einem langen Format transformiert, wobei jede Kennzahl in einer eigenen Zeile dargestellt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zugangsdaten für PostgreSQL Server und Datenbank\n",
    "host = \"localhost\"\n",
    "port = \"5432\"\n",
    "database = \"studienarbeit_dateninfrastruktur\"\n",
    "user = \"postgres\"\n",
    "password = \"5566\"\n",
    "\n",
    "# Verbindungsstring mit Datenbankdetails zusammenstellen\n",
    "connection_string = f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}\"\n",
    "engine = create_engine(connection_string)\n",
    "metadata = MetaData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'dimensions' Dictionary initialisieren\n",
    "dimensions = {}\n",
    "\n",
    "# Kombinieren aller DataFrame-Regioneninformationen\n",
    "region_data = []\n",
    "for df in data_frames.values():\n",
    "    if {'_rs', 'name', 'regionalebene'}.issubset(df.columns):\n",
    "        region_data.append(df[['_rs', 'name', 'regionalebene']].drop_duplicates().reset_index(drop=True))\n",
    "        \n",
    "region_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überführen in 2. Normalform und Erstellen der Dimensionstabellen im Metadata.\n",
    "# Alle Nicht-Schlüsselattribute hängen vollständig vom Primärschlüssel ab.\n",
    "# Dimensionstabellen definieren\n",
    "dim_regionen = Table('dim_regionen', metadata,\n",
    "    Column('region_id', String, primary_key=True),\n",
    "    Column('name', String),\n",
    "    Column('regionalebene', String)\n",
    ")\n",
    "# Erstellen der einheitlichen Dimensionstabelle für Regionen\n",
    "combined_region_data = pd.concat(region_data).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "combined_region_data.rename(columns={'_rs': 'region_id'}, inplace=True)  # Hier '_rs' in 'region_id' umbenennen\n",
    "dimensions['dim_regionen'] = combined_region_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_region_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen der Dimensionstabelle für Berichtszeitpunkte\n",
    "all_dates = pd.concat([df['berichtszeitpunkt'] for df in data_frames.values() if 'berichtszeitpunkt' in df.columns], ignore_index=True).drop_duplicates()\n",
    "# Convert to datetime and back to string to ensure valid dates and consistent formatting\n",
    "all_dates = pd.to_datetime(all_dates, format='%Y%m%d').dt.strftime('%Y%m%d')\n",
    "dimensions['dim_zeitpunkte'] = pd.DataFrame({\n",
    "    'berichtszeitpunkt': all_dates,\n",
    "    'time_id': range(1, len(all_dates) + 1)\n",
    "})\n",
    "\n",
    "# Erstellen der Dimensionstabelle für Geschlecht\n",
    "geschlecht_data = pd.DataFrame({\n",
    "    'geschlecht': ['personen_total', 'männlich', 'weiblich'],\n",
    "    'geschlecht_id': [1, 2, 3]\n",
    "})\n",
    "dimensions['dim_geschlecht'] = geschlecht_data\n",
    "\n",
    "# Erstellen der Dimensionstabelle für Entitäten\n",
    "dimensions['dim_entitaeten'] = pd.DataFrame({\n",
    "    'entitaet': list(data_frames.keys()),\n",
    "    'entitaet_id': range(1, len(data_frames) + 1)\n",
    "})\n",
    "\n",
    "# Erstellen der Dimensionstabelle für Attributdetails\n",
    "attribute_details_df['attribut_id'] = range(1, len(attribute_details_df) + 1)\n",
    "attribute_details_df['attribut'] = attribute_details_df['Attribut'].str.lower()\n",
    "# Cleaning special characters and spaces in the dictionary keys\n",
    "attribute_details_df['attribut'] = attribute_details_df['attribut'].str.replace('–', '').str.strip()\n",
    "attribute_details_df['detail'] = attribute_details_df['Detail'].str.lower()\n",
    "dimensions['dim_attribute_details'] = attribute_details_df[['attribut_id', 'attribut', 'detail']]\n",
    "\n",
    "# Create the dictionary for attribute ID mapping\n",
    "attribute_details_dict = attribute_details_df.set_index('attribut')['attribut_id'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly create dimension tables in the SQL database, even if they are empty\n",
    "for table_name, df in dimensions.items():\n",
    "    print(f\"Überprüfung der Tabelle {table_name}, DataFrame hat {len(df)} Zeilen.\")\n",
    "    \n",
    "    # Wenn der DataFrame leer ist, sicherstellen, dass die Tabelle trotzdem erstellt wird\n",
    "    if df.empty:\n",
    "        # Erstellen des Tabellenschemas ohne Daten einzufügen\n",
    "        pd.io.sql.get_schema(df, table_name, con=engine)\n",
    "        print(f\"Tabellenschema für {table_name} wurde erstellt, da es leer war.\")\n",
    "    else:\n",
    "        # Daten in die entsprechende Tabelle laden\n",
    "        df.to_sql(table_name, con=engine, if_exists='replace', index=False)\n",
    "        print(f\"Dimensionstabelle {table_name} wurde erfolgreich hochgeladen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faktendaten = []\n",
    "\n",
    "for sheet, df in data_frames.items():\n",
    "    df['entitaet_id'] = dimensions['dim_entitaeten'].set_index('entitaet').loc[sheet, 'entitaet_id']\n",
    "    id_vars = ['_rs', 'name', 'regionalebene', 'entitaet_id']\n",
    "    if 'berichtszeitpunkt' in df.columns:\n",
    "        id_vars.append('berichtszeitpunkt')\n",
    "        df['berichtszeitpunkt'] = df['berichtszeitpunkt'].astype(str)\n",
    "\n",
    "    value_vars = [col for col in df.columns if col not in id_vars and not col.endswith('_detail')]\n",
    "\n",
    "    # Entpivotieren\n",
    "    df_long = pd.melt(df, id_vars=id_vars, value_vars=value_vars, var_name='kennzahlentyp', value_name='kennzahlenwert')\n",
    "\n",
    "    if 'berichtszeitpunkt' in df_long.columns:\n",
    "        df_long['berichtszeitpunkt'] = df_long['berichtszeitpunkt'].astype(str)\n",
    "        df_long = df_long.merge(\n",
    "            dimensions['dim_zeitpunkte'][['berichtszeitpunkt', 'time_id']],\n",
    "            on='berichtszeitpunkt',\n",
    "            how='left') \n",
    "\n",
    "    df_long['kennzahlentyp'] = df_long['kennzahlentyp'].str.strip().str.lower()\n",
    "    df_long['attribut_id'] = df_long['kennzahlentyp'].map(attribute_details_dict).fillna(0).astype(int) \n",
    "\n",
    "    df_long = df_long.merge(dimensions['dim_attribute_details'][['attribut_id', 'detail']], on='attribut_id', how='left')\n",
    "\n",
    "    # Bestimmung des Geschlechts basierend auf 'kennzahlentyp'\n",
    "    df_long['geschlecht'] = df_long['kennzahlentyp'].apply(\n",
    "        lambda x: 'personen_total' if '_M' not in x and '_W' not in x else ('männlich' if '_M' in x else 'weiblich')\n",
    "    )\n",
    "    df_long = df_long.merge(dimensions['dim_geschlecht'], on='geschlecht', how='left')\n",
    "\n",
    "    if '_rs' in df_long.columns:\n",
    "        df_long.rename(columns={'_rs': 'region_id'}, inplace=True)\n",
    "\n",
    "    df_long = df_long.merge(dimensions['dim_regionen'], on=['region_id', 'name', 'regionalebene'], how='left')\n",
    "\n",
    "    # Drop columns only if they exist in the DataFrame\n",
    "    columns_to_drop = ['_rs', 'name', 'regionalebene', 'berichtszeitpunkt', 'geschlecht', 'attribut_id']\n",
    "    df_long.drop(columns=[col for col in columns_to_drop if col in df_long.columns], inplace=True)\n",
    "\n",
    "    # 'kennzahlenwert' zu numerischen Werten konvertieren und ungültige Einträge entfernen\n",
    "    df_long['kennzahlenwert'] = pd.to_numeric(df_long['kennzahlenwert'], errors='coerce')\n",
    "    df_long.dropna(subset=['kennzahlenwert'], inplace=True)\n",
    "    df_long.columns = [col.lower() for col in df_long.columns]\n",
    "        \n",
    "    faktendaten.append(df_long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zusammenführen aller Daten in einer einzigen Faktentabelle\n",
    "fct_zensus = pd.concat(faktendaten, ignore_index=True)\n",
    "\n",
    "# Definition und Erstellung der Faktentabelle in der Datenbank\n",
    "fct_zensus_table = Table('fct_zensus', metadata,\n",
    "    Column('index', Integer, primary_key=True),\n",
    "    Column('kennzahlentyp', String),\n",
    "    Column('detail', String),\n",
    "    Column('kennzahlenwert', Integer),\n",
    "    Column('region_id', String),\n",
    "    Column('time_id', Integer),\n",
    "    Column('entitaet_id', Integer),\n",
    "    Column('geschlecht_id', Integer),\n",
    "    Column('attribut_id', Integer),\n",
    "    extend_existing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen der Tabellen und Laden der Daten in die Datenbank\n",
    "metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fact table data\n",
    "fct_zensus.to_sql('fct_zensus', engine, if_exists='replace', index=False)\n",
    "\n",
    "# Prüfung der finalen Struktur der Faktentabelle in der Datenbank\n",
    "query_columns = \"SELECT * FROM fct_zensus LIMIT 0;\"\n",
    "df_columns = pd.read_sql(query_columns, engine)\n",
    "print(\"Spalten in fct_zensus nach Update:\")\n",
    "print(df_columns.columns.tolist())\n",
    "\n",
    "print(\"Alle Daten wurden erfolgreich in die Datenbank geladen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d: Visualisierung der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locale.setlocale(locale.LC_ALL, 'de_DE.UTF-8')\n",
    "try:\n",
    "    locale.setlocale(locale.LC_ALL, 'de_DE.UTF-8')\n",
    "except Exception as e:\n",
    "    print(f\"Locale setting failed: {e}\")\n",
    "\n",
    "# Define the SQL query\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    r.name AS region, \n",
    "    d.detail AS erwerbsstatus,\n",
    "    g.geschlecht,\n",
    "    SUM(f.kennzahlenwert) AS summe\n",
    "FROM \n",
    "    fct_zensus f\n",
    "JOIN \n",
    "    dim_regionen r ON f.region_id = r.region_id\n",
    "JOIN \n",
    "    dim_entitaeten e ON f.entitaet_id = e.entitaet_id AND e.entitaet_id = 5\n",
    "JOIN \n",
    "    dim_geschlecht g ON f.geschlecht_id = g.geschlecht_id\n",
    "JOIN \n",
    "    dim_attribute_details d ON f.attribut_id = d.attribut_id\n",
    "WHERE \n",
    "    r.regionalebene = 'Land'  -- Focuses the query on the state level\n",
    "GROUP BY \n",
    "    r.name, d.detail, g.geschlecht\n",
    "ORDER BY \n",
    "    r.name, d.detail, g.geschlecht;\n",
    "\"\"\"\n",
    "\n",
    "# Daten laden\n",
    "try:\n",
    "    data = pd.read_sql(query, engine)\n",
    "    print(data.head())\n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Laden der Daten: {e}\")\n",
    "\n",
    "# Check if data is empty\n",
    "if not data.empty:\n",
    "    # Pivotiere die Daten für das Stacked Bar Chart\n",
    "    pivot_data = data.pivot_table(index='region', columns=['erwerbsstatus', 'geschlecht'], values='summe', aggfunc='sum')\n",
    "    pivot_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Erstelle ein Balkendiagramm\n",
    "    fig, ax = plt.subplots(figsize=(16, 9))\n",
    "    pivot_data.plot(kind='bar', stacked=True, ax=ax, colormap='viridis')\n",
    "\n",
    "    # Beschriftungen und Titel anpassen\n",
    "    ax.set_xlabel('Bundesländer', fontsize=12)\n",
    "    ax.set_ylabel('Anzahl Erwerbstätige', fontsize=12)\n",
    "    ax.set_title('Erwerbsstatus nach Regionalebene \"Land\"', fontsize=14)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.legend(title='Erwerbsstatus', fontsize=10, loc='upper left')\n",
    "\n",
    "    # Linien an den Außenrändern entfernen\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # Y-Achse dynamisch skalieren\n",
    "    max_y_value = pivot_data.max().max()\n",
    "    ax.set_ylim(0, max_y_value * 1.1)\n",
    "    ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Keine Daten vorhanden, um das Diagramm zu erstellen.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Studienarbeit-H_ezNwIf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
